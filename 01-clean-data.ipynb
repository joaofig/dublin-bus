{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dublin Buses - Clean Data\n",
    "\n",
    "Prerequisites: `00-download-data.ipynb`\n",
    "\n",
    "Before running the code in this notebook, you must download and concatenate all the original per-day data files into a single parquet file. Please use the above notebook to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "import osmnx as ox\n",
    "from ipywidgets import interact, interact_manual\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import folium\n",
    "import multiprocessing\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.neighbors import BallTree\n",
    "\n",
    "from geo.geomath import vec_haversine, num_haversine\n",
    "from geo.df import DataCleaner\n",
    "from par.allel import parallel_process\n",
    "from geo.df import mem_usage, categorize_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the data in parquet format, as generated by the first step. Note that not all columns are being read in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_read = ['Timestamp', 'LineID', 'Direction', 'PatternID', \n",
    "                   'JourneyID', 'Congestion', 'Lon', 'Lat', \n",
    "                   'Delay', 'BlockID', 'VehicleID', 'StopID', 'AtStop']\n",
    "df = pd.read_parquet(\"data/sir010113-310113.parquet\", columns=columns_to_read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 4997.05 MB'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mem_usage(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 2586.19 MB'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = categorize_columns(df, ['PatternID'])\n",
    "mem_usage(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "journeys = df.JourneyID.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18614,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "journeys.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaner = DataCleaner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicles = df['VehicleID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_vehicles = df['VehicleID'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_runs(a):\n",
    "    # Source: https://stackoverflow.com/questions/24885092/finding-the-consecutive-zeros-in-a-numpy-array\n",
    "    # Create an array that is 1 where a is 0, and pad each end with an extra 0.\n",
    "    iszero = np.concatenate(([0], np.equal(a, 0).view(np.int8), [0]))\n",
    "    absdiff = np.abs(np.diff(iszero))\n",
    "    # Runs start and end where absdiff is 1.\n",
    "    ranges = np.where(absdiff == 1)[0].reshape(-1, 2)\n",
    "    return ranges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `get_top_whisker_speed` function calculates the speed corresponding to the top whisker on a box-and-whiskers plot, using Tukey's formulation. The top whisker corresponds to 1.5 times the interquartile range added to the third quartile value. Use this function to calculate the most likely top speed on a per-vehicle basis, when fixing the type-2 anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_whisker_speed(df):\n",
    "    q = df['v'].quantile([.25, .5, .75])\n",
    "    iqr = q.loc[0.75] - q.loc[0.25]\n",
    "    return q.loc[0.75] + 1.5 * iqr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `process_vehicle` function fixes all anomalies on a vehicle partition of the data. After fixing the anomalies, the function returns a dictionary containing the vehicle identifier, the cleaned-up DataFrame, the type 2 anomalies DataFrame, and the _top whisker speed_ value for the vehicle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_vehicle(v, df):\n",
    "    df = cleaner.calculate_derived_columns(df)\n",
    "    df = cleaner.fix_type1_anomalies(df)\n",
    "    max_v = get_top_whisker_speed(df)\n",
    "    df, anomalies = cleaner.fix_type2_anomalies(df, max_speed=max_v)\n",
    "    return {'v': v, 'df': df, 'anom': anomalies, 'max_v': max_v }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an array with the input data for the parallel anomaly correction process. Each array element contains the required parameters for a call to the `process_vehicle` function. Having the data split up by vehicle helps in the parallelization process. Each process looks at a single partition of the data, avoiding data concurrency problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14405117e52c4233a923c31d82b69aeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=911), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "vehicle_data = [{'v': v, 'df': df[df['VehicleID'] == v].copy().sort_values(by='Timestamp')} for v in tqdm(unique_vehicles)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not need the main DataFrame anymore, so we can do away with it and save some precious memory in the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the parallel process that fixes all the anomalies. Note that this piece of code can take a long time to run, depending on your hardware. The more cores, the better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d78521652d24156acc0606ce8623017",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=908), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2670fc2af13415eb4ab8bbff7d8f4ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "fixed_data = parallel_process(vehicle_data, process_vehicle, use_kwargs=True, tqdm=tqdm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the sub folders in the `data` folder to receive the fixed data and the anomaly data _per vehicle_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists(\"data/fixed\"):\n",
    "    os.makedirs(\"data/fixed\")\n",
    "    \n",
    "if not os.path.exists(\"data/anomaly\"):\n",
    "    os.makedirs(\"data/anomaly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save both the fixed per-vehicle DataFrames and the anomaly points. These might still be of use in the future, who knows?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb0c272e4636423185a5e608c275733a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=911), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for vd in tqdm(fixed_data):\n",
    "    if isinstance(vd, dict):\n",
    "        v = vd['v']\n",
    "        df = vd['df']\n",
    "        anom = vd['anom']\n",
    "        df.to_parquet(\"data/fixed/v_{0}.parquet\".format(v), index=False)\n",
    "        \n",
    "        if anom is not None:\n",
    "            anom.to_parquet(\"data/anomaly/a_{0}.parquet\".format(v), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the bounding box of the samples and save it on a JSON text file for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0d257c380cb421dbd64548c6ccefae2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=911), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import json\n",
    "\n",
    "min_lat = sys.float_info.max\n",
    "max_lat = -sys.float_info.max\n",
    "min_lon = sys.float_info.max\n",
    "max_lon = -sys.float_info.max\n",
    "\n",
    "for vd in tqdm(fixed_data):\n",
    "    if isinstance(vd, dict):\n",
    "        v = vd['v']\n",
    "        df = vd['df']\n",
    "        anom = vd['anom']       \n",
    "        min_lat = min(min_lat, df['Lat'].min())\n",
    "        max_lat = max(max_lat, df['Lat'].max())\n",
    "        min_lon = min(min_lon, df['Lon'].min())\n",
    "        max_lon = max(max_lon, df['Lon'].max())\n",
    "\n",
    "bbox = {'west': min_lon, 'east': max_lon, 'north': max_lat, 'south': min_lat}\n",
    "\n",
    "with open('data/bbox.txt', 'w') as json_file:\n",
    "  json.dump(bbox, json_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
