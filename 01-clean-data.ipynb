{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dublin Buses - Clean Data\n",
    "\n",
    "Prerequisites: `00-download-data.ipynb`\n",
    "\n",
    "Before running the code in this notebook, you must download and concatenate all the original per-day data files into a single parquet file. Please use the above notebook to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "import osmnx as ox\n",
    "from ipywidgets import interact, interact_manual\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import folium\n",
    "import multiprocessing\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.neighbors import BallTree\n",
    "\n",
    "from geo.geomath import vec_haversine, num_haversine\n",
    "from geo.df import DataCleaner\n",
    "from par.allel import parallel_process\n",
    "from geo.df import mem_usage, categorize_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the data in parquet format, as generated by the first step. Note that not all columns are being read in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_read = ['Timestamp', 'LineID', 'Direction', 'PatternID', \n",
    "                   'JourneyID', 'Congestion', 'Lon', 'Lat', \n",
    "                   'Delay', 'BlockID', 'VehicleID', 'StopID', 'AtStop']\n",
    "df = pd.read_parquet(\"data/sir010113-310113.parquet\", columns=columns_to_read)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the DataFrame memory consumption as read from storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 4997.05 MB'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mem_usage(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it stands, the `PatternID` column can be converted from string to categorical, saving us some precious memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 2586.19 MB'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = categorize_columns(df, ['PatternID'])\n",
    "mem_usage(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, that was an almost 50% reduction in memory consumption!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Contents\n",
    "Let us see what the DataFrame contains before any data cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44455133"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18614"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "journeys = df.JourneyID.unique()\n",
    "journeys.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "911"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vehicles = df['VehicleID'].unique()\n",
    "vehicles.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4728"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stops = df['StopID'].unique()\n",
    "stops.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Quality\n",
    "Here we will synthesize some of the missing features, such as the traveled distance and the average speed. This is done using a naÃ¯ve approach. We start by calculating the time differences between consecutive observations. Next, we calculate the distance between these points, and finally we can infer the average speed.\n",
    "\n",
    "Let's start off by sorting the DataFrame using the `VehicleID` and `Timestamp` as keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(by=['VehicleID', 'Timestamp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we extract a series containing the sequence of vehicle identifiers. This will be useful to mark the rows that border consecutive vehicles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicle_df = df['VehicleID']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't want to keep the distance between the last point of one vehicle, recorded at the end of the period, and the first point of the next vehicle, which will be recorded at the start of the next period. These values do not make any sense and will be later set to zero. This series will help us determine when to discard a consecutive calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duration\n",
    "Let's start by calculating the consecutive durations in seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['dt'] = df['Timestamp'].diff()\n",
    "df['dt'] = df['dt'].fillna(value=0.0)\n",
    "df['dt'] = df['dt'] / 1000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance\n",
    "Now we can calculate the consecutive distances in meters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat0 = df['Lat'][:-1].to_numpy()\n",
    "lon0 = df['Lon'][:-1].to_numpy()\n",
    "lat1 = df['Lat'][1:].to_numpy()\n",
    "lon1 = df['Lon'][1:].to_numpy()\n",
    "dist = vec_haversine(lat0, lon0, lat1, lon1)\n",
    "df['dx'] = np.insert(dist, 0, 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we must correct the consecutive durations and distances by setting to zero the ones at the beginning of each vehicle sequence. Please remember that these do not make sense as they are measuring durations and distances between different vehicles. The code to do this is simple enough:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[vehicle_df.diff() != 0, ['dt', 'dx']] = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reasoning behind this code is simple: find the rows whose consecutive difference of the vehicle identifier is not zero (this should include the initial `NaN`) and set both distance and duration to zero. Using this simple trick, we were able to process all vehicles with single vectorized operations and skipped grouping altogether."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speed\n",
    "Now that we have consecutive distances and durations, why not have a go and estimate the speed? The calculation is easy enough, just divide each non-zero distance by its corresponding duration and we get the estimated average speed between two consecutive points, what can go wrong?\n",
    "\n",
    "Again, we will be using vectorization to avoid lengthy computations. Here's the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dx = df['dx'].to_numpy()\n",
    "dt = df['dt'].to_numpy()\n",
    "v = np.zeros_like(dx)\n",
    "zi = dt > 0\n",
    "v[zi] = dx[zi] / dt[zi] * 3.6\n",
    "df['v'] = v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trick here is to compute all the array indexes where the time difference is not zero. Once that is done the average speed is readily computed. The speed unit is kilometers per hour so we can get an intuitive sense of what is happening.\n",
    "\n",
    "Why not have a look at the average speed distribution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "vbx = df['v'].plot.box(grid=True, figsize=(6,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaner = DataCleaner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_runs(a):\n",
    "    # Source: https://stackoverflow.com/questions/24885092/finding-the-consecutive-zeros-in-a-numpy-array\n",
    "    # Create an array that is 1 where a is 0, and pad each end with an extra 0.\n",
    "    iszero = np.concatenate(([0], np.equal(a, 0).view(np.int8), [0]))\n",
    "    absdiff = np.abs(np.diff(iszero))\n",
    "    # Runs start and end where absdiff is 1.\n",
    "    ranges = np.where(absdiff == 1)[0].reshape(-1, 2)\n",
    "    return ranges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `get_top_whisker_speed` function calculates the speed corresponding to the top whisker on a box-and-whiskers plot, using Tukey's formulation. The top whisker corresponds to 1.5 times the interquartile range added to the third quartile value. Use this function to calculate the most likely top speed on a per-vehicle basis, when fixing the type-2 anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_whisker_speed(df):\n",
    "    q = df['v'].quantile([.25, .5, .75])\n",
    "    iqr = q.loc[0.75] - q.loc[0.25]\n",
    "    return q.loc[0.75] + 1.5 * iqr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `process_vehicle` function fixes all anomalies on a vehicle partition of the data. After fixing the anomalies, the function returns a dictionary containing the vehicle identifier, the cleaned-up DataFrame, the type 2 anomalies DataFrame, and the _top whisker speed_ value for the vehicle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_vehicle(v, df):\n",
    "    df = cleaner.calculate_derived_columns(df)\n",
    "    df = cleaner.fix_type1_anomalies(df)\n",
    "    max_v = get_top_whisker_speed(df)\n",
    "    df, anomalies = cleaner.fix_type2_anomalies(df, max_speed=max_v)\n",
    "    return {'v': v, 'df': df, 'anom': anomalies, 'max_v': max_v }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an array with the input data for the parallel anomaly correction process. Each array element contains the required parameters for a call to the `process_vehicle` function. Having the data split up by vehicle helps in the parallelization process. Each process looks at a single partition of the data, avoiding data concurrency problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicle_data = [{'v': v, 'df': df[df['VehicleID'] == v].copy().sort_values(by='Timestamp')} for v in tqdm(vehicles)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not need the main DataFrame anymore, so we can do away with it and save some precious memory in the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the parallel process that fixes all the anomalies. Note that this piece of code can take a long time to run, depending on your hardware. The more cores, the better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_data = parallel_process(vehicle_data, process_vehicle, use_kwargs=True, tqdm=tqdm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the sub folders in the `data` folder to receive the fixed data and the anomaly data _per vehicle_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists(\"data/fixed\"):\n",
    "    os.makedirs(\"data/fixed\")\n",
    "    \n",
    "if not os.path.exists(\"data/anomaly\"):\n",
    "    os.makedirs(\"data/anomaly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save both the fixed per-vehicle DataFrames and the anomaly points. These might still be of use in the future, who knows?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for vd in tqdm(fixed_data):\n",
    "    if isinstance(vd, dict):\n",
    "        v = vd['v']\n",
    "        df = vd['df']\n",
    "        anom = vd['anom']\n",
    "        df.to_parquet(\"data/fixed/v_{0}.parquet\".format(v), index=False)\n",
    "        \n",
    "        if anom is not None:\n",
    "            anom.to_parquet(\"data/anomaly/a_{0}.parquet\".format(v), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the bounding box of the samples and save it on a JSON text file for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "\n",
    "min_lat = sys.float_info.max\n",
    "max_lat = -sys.float_info.max\n",
    "min_lon = sys.float_info.max\n",
    "max_lon = -sys.float_info.max\n",
    "\n",
    "for vd in tqdm(fixed_data):\n",
    "    if isinstance(vd, dict):\n",
    "        v = vd['v']\n",
    "        df = vd['df']\n",
    "        anom = vd['anom']       \n",
    "        min_lat = min(min_lat, df['Lat'].min())\n",
    "        max_lat = max(max_lat, df['Lat'].max())\n",
    "        min_lon = min(min_lon, df['Lon'].min())\n",
    "        max_lon = max(max_lon, df['Lon'].max())\n",
    "\n",
    "bbox = {'west': min_lon, 'east': max_lon, 'north': max_lat, 'south': min_lat}\n",
    "\n",
    "with open('data/bbox.txt', 'w') as json_file:\n",
    "  json.dump(bbox, json_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
