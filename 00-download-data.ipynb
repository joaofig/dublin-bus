{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dublin Buses - Download Data\n",
    "\n",
    "This notebook downloads and consolidates the 31 Dublin Bus daily datasets into a single parquet file. The dataset is freely provided by the [Smart Dublin](https://data.smartdublin.ie/dataset/dublin-bus-gps-sample-data-from-dublin-city-council-insight-project) website. It contains the GPS traces of the Dublin buses for January 2013.\n",
    "\n",
    "Notes:\n",
    "* Please install the `pyarrow` package so you can use the parquet file format\n",
    "* The final DataFrame is quite large and may impose some memory pressure on your machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "import urllib\n",
    "import os\n",
    "from ipywidgets import interact, interact_manual\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the file names for later use. The `data` folder may not yet exist on your project, but it will be created in due course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_file_name = \"data/sir010113-310113.zip\"\n",
    "parquet_file_name = \"data/sir010113-310113.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_zip():\n",
    "    url = \"http://opendata.dublincity.ie/TrafficOpenData/sir010113-310113.zip\"\n",
    "    urllib.request.urlretrieve(url, zip_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `read_data_frame` function reads each of the daily datasets into a Pandas DataFrame. These are then concatenated into a single DataFrame and saved to storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_frame(filename):\n",
    "    header = ['Timestamp', 'LineID', 'Direction', 'PatternID', 'TimeFrame', \n",
    "              'JourneyID', 'Operator', 'Congestion', 'Lon', 'Lat', \n",
    "              'Delay', 'BlockID', 'VehicleID', 'StopID', 'AtStop']   \n",
    "    types = {'Timestamp': np.int64,\n",
    "             'JourneyID': np.int32,\n",
    "             'Congestion': np.int8,\n",
    "             'Lon': np.float64,\n",
    "             'Lat': np.float64,\n",
    "             'Delay': np.int8,\n",
    "             'VehicleID': np.int32,\n",
    "             'AtStop': np.int8}\n",
    "    df = pd.read_csv(filename, header=None, names=header, dtype=types, \n",
    "                     parse_dates=['TimeFrame'], infer_datetime_format=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `prepare_data_frame` operates on the final DataFrame to make `NaN` replacements and final type changes that were not possible during reading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_frame(df): \n",
    "    null_replacements = {'LineID': 0, 'StopID': 0}\n",
    "    df = df.fillna(value=null_replacements)\n",
    "    df['LineID'] = df['LineID'].astype(np.int32)\n",
    "    df['StopID'] = df['StopID'].astype(np.int32)\n",
    "    df['DateTime'] = pd.to_datetime(df['Timestamp'], unit='us')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `read_zip_file` function reads in in all the extracted files into a single DataFrame. The generator expression makes the concatenation quite swift and precludes the use of other supporting variables. Note that as previously mentioned, this function may exert some pressure on your memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_zip_file(filename):\n",
    "    final_df = None\n",
    "    file_names = []\n",
    "    \n",
    "    print(\"Unzipping:\")\n",
    "    with zipfile.ZipFile(filename) as z:\n",
    "        files = z.infolist()\n",
    "        for f in tqdm(files):\n",
    "            z.extract(f, path='data')\n",
    "            file_names.append(\"data/\" + f.filename)\n",
    "    \n",
    "    print(\"Concatenating:\")\n",
    "    df = pd.concat((read_data_frame(file) for file in tqdm(file_names)), ignore_index=True)\n",
    "    \n",
    "    print(\"Deleting:\")\n",
    "    for file in tqdm(file_names):\n",
    "        os.remove(file)\n",
    "        \n",
    "    print(\"Terminating...\")\n",
    "    df = prepare_data_frame(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the `data` directory if it does not yet exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"data\"):\n",
    "    os.makedirs(\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conditionally download and process the data file. The download process can take a *long time*, so please make sure to retain the zip file for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(zip_file_name):\n",
    "    download_zip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unzipping:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5b1be181f1c4c12b473ecffd8ee9358",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=31), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Concatenating:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fa800b151904723a8af39517b5d859e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=31), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Deleting:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e32adb76eb048d0833fb0106fc6b5c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=31), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Terminating...\n"
     ]
    }
   ],
   "source": [
    "df = read_zip_file(zip_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the consolidated DataFrame to a parquet-formatted file. This is where you need the `pyarrow` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(parquet_file_name):\n",
    "    df.to_parquet(parquet_file_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are done. Please proceed to the next notebook to clean up the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
